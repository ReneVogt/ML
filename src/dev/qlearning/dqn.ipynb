{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import importlib\n",
    "import tictactoe\n",
    "importlib.reload(tictactoe);\n",
    "from tictactoe import TicTacToe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Hyper parameters\n",
    "# \n",
    "alpha = 0.4\n",
    "gamma = 0.9\n",
    "epsilon = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create the model and optimizer\n",
    "# \n",
    "model = nn.Sequential(\n",
    "    nn.Linear(27, 81),    \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(81, 81),    \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(81, 9)\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=alpha)\n",
    "games = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#validation\n",
    "#\n",
    "def validate(iterations):\n",
    "    train = model.training\n",
    "    model.eval()\n",
    "    wins = 0\n",
    "    draws = 0\n",
    "    losses = 0\n",
    "    for _ in range(iterations):\n",
    "        env = TicTacToe()\n",
    "        qplayer = random.choice([env.player, env.opponent])\n",
    "        done = False\n",
    "        while not done:\n",
    "            state = env.state\n",
    "            if qplayer == env.player:\n",
    "                q = model(state)\n",
    "                action = max([a for a in range(9) if env.is_valid(a)], key = lambda x: q[x])\n",
    "            else:\n",
    "                action = random.choice([a for a in range(9) if env.is_valid(a)])\n",
    "            env.move(action)\n",
    "            if env.is_won():\n",
    "                if qplayer == env.opponent:\n",
    "                    wins += 1\n",
    "                else:\n",
    "                    losses += 1\n",
    "                done = True\n",
    "            elif env.is_full():\n",
    "                draws += 1\n",
    "                done = True\n",
    "\n",
    "    print(f'Games: {iterations} Wins: {wins} Losses: {losses} Draws: {draws} ====> {100*(wins+draws)/iterations:.2f}%')\n",
    "    if train:\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000: average loss: 0.027122744375187902\n",
      "20000: average loss: 0.03113090331145795\n",
      "20000: validating...\n",
      "Games: 10000 Wins: 6113 Losses: 3427 Draws: 460 ====> 65.73%\n",
      "25000: average loss: 0.07568025459446945\n",
      "30000: average loss: 0.02609849924442824\n",
      "30000: validating...\n",
      "Games: 10000 Wins: 5193 Losses: 4329 Draws: 478 ====> 56.71%\n",
      "35000: average loss: 0.026384221484162845\n",
      "40000: average loss: 0.02969073413410224\n",
      "40000: validating...\n",
      "Games: 10000 Wins: 4667 Losses: 3919 Draws: 1414 ====> 60.81%\n",
      "45000: average loss: 0.04185481670740992\n",
      "50000: average loss: 0.026384277726802977\n",
      "50000: validating...\n",
      "Games: 10000 Wins: 5036 Losses: 3376 Draws: 1588 ====> 66.24%\n",
      "55000: average loss: 0.026876952226296998\n",
      "60000: average loss: 0.026693575509032234\n",
      "60000: validating...\n",
      "Games: 10000 Wins: 3142 Losses: 4463 Draws: 2395 ====> 55.37%\n",
      "65000: average loss: 0.027314657233119943\n",
      "70000: average loss: 0.026650017789262348\n",
      "70000: validating...\n",
      "Games: 10000 Wins: 3569 Losses: 3405 Draws: 3026 ====> 65.95%\n",
      "75000: average loss: 0.027799547301395795\n",
      "80000: average loss: 0.0268265156397596\n",
      "80000: validating...\n",
      "Games: 10000 Wins: 6135 Losses: 3192 Draws: 673 ====> 68.08%\n",
      "85000: average loss: 0.026341476899641565\n",
      "90000: average loss: 0.027116406166856177\n",
      "90000: validating...\n",
      "Games: 10000 Wins: 3181 Losses: 3656 Draws: 3163 ====> 63.44%\n",
      "95000: average loss: 0.028357113884412685\n",
      "100000: average loss: 0.026897025150898844\n",
      "100000: validating...\n",
      "Games: 10000 Wins: 4028 Losses: 4771 Draws: 1201 ====> 52.29%\n",
      "105000: average loss: 0.0267764011553023\n",
      "110000: average loss: 0.027210236174613236\n",
      "110000: validating...\n",
      "Games: 10000 Wins: 3041 Losses: 4800 Draws: 2159 ====> 52.00%\n",
      "115000: average loss: 0.027484344375459477\n",
      "120000: average loss: 0.027271392260305585\n",
      "120000: validating...\n",
      "Games: 10000 Wins: 4790 Losses: 3132 Draws: 2078 ====> 68.68%\n",
      "125000: average loss: 0.02652672769653145\n",
      "130000: average loss: 0.026746161706582644\n",
      "130000: validating...\n",
      "Games: 10000 Wins: 4110 Losses: 4398 Draws: 1492 ====> 56.02%\n",
      "135000: average loss: 0.026310859548172447\n",
      "140000: average loss: 0.02708772152240854\n",
      "140000: validating...\n",
      "Games: 10000 Wins: 5598 Losses: 3938 Draws: 464 ====> 60.62%\n",
      "145000: average loss: 0.02631428042636253\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m     36\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 37\u001b[0m     next_q \u001b[39m=\u001b[39m model(env\u001b[39m.\u001b[39;49mstate)\n\u001b[0;32m     38\u001b[0m     next_max \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39mmax\u001b[39m([next_q[a] \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m9\u001b[39m) \u001b[39mif\u001b[39;00m env\u001b[39m.\u001b[39mis_valid(a)])\n\u001b[0;32m     39\u001b[0m model\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\rvogt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rvogt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rvogt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rvogt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#\n",
    "# TRAINING\n",
    "#\n",
    "log_interval = 5000\n",
    "eval_interval = 10000\n",
    "eval_iterations = 10000\n",
    "losses = []\n",
    "model.train()\n",
    "for _ in range(2000000):\n",
    "    env = TicTacToe()\n",
    "    done = False\n",
    "    games += 1\n",
    "    loss = 0\n",
    "    moves = 0\n",
    "\n",
    "    while not done:\n",
    "        state = env.state\n",
    "        q = model(state)\n",
    "        targetq = q.detach().clone()\n",
    "\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice([a for a in range(9) if env.is_valid(a)])\n",
    "        else:\n",
    "            action = max([a for a in range(9) if env.is_valid(a)], key=lambda x: q[x])\n",
    "        \n",
    "        env.move(action)\n",
    "\n",
    "        if env.is_won():\n",
    "            targetq[action] = 1\n",
    "            done = True\n",
    "        elif env.is_full():\n",
    "            targetq[action] = 0\n",
    "            done = True\n",
    "        else:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                next_q = model(env.state)\n",
    "                next_max = -max([next_q[a] for a in range(9) if env.is_valid(a)])\n",
    "            model.train()\n",
    "            targetq[action] = -0.1 + gamma * next_max\n",
    "            \n",
    "        loss += F.mse_loss(q, targetq)\n",
    "        moves += 1\n",
    "\n",
    "    loss /= moves\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if games % log_interval == 0:\n",
    "        print(f'{games}: average loss: {sum(losses)/len(losses)}')\n",
    "        losses = []\n",
    "    if games % eval_interval == 0:\n",
    "        print(f'{games}: validating...')\n",
    "        validate(eval_iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TicTacToe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.move(4)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.state\n",
    "q = model(state)\n",
    "action = max([a for a in range(9) if env.is_valid(a)], key=lambda x: q[x])\n",
    "env.move(action)\n",
    "print(action)\n",
    "env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
