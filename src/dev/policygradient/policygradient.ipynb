{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import importlib\n",
    "import connect4\n",
    "importlib.reload(connect4);\n",
    "from connect4 import Connect4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Hyper parameters\n",
    "# \n",
    "alpha = 0.005\n",
    "gamma = 0.99\n",
    "entropy_coefficient = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create the model and optimizer\n",
    "# \n",
    "model = nn.Sequential(\n",
    "    nn.Linear(126, 294),\n",
    "#    nn.ReLU(),\n",
    "#    nn.Linear(294, 294),\n",
    "    nn.LayerNorm(294),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(294, 7),\n",
    "    nn.Softmax(dim=-1))\n",
    "games = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr = alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load model from checkpoint\n",
    "#\n",
    "games = 250000\n",
    "cp = torch.load(f'connect4-{games}.nn');\n",
    "model.load_state_dict(cp['model_state_dict']);\n",
    "optimizer.load_state_dict(cp['optimizer_state_dict']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def checkpoint(step):\n",
    "    train = model.training\n",
    "    if train: model.eval();\n",
    "    print(f\"{step}: checkpoint...\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, f'connect4-{step}.nn');\n",
    "\n",
    "    dummy_input = Connect4().state\n",
    "    torch.onnx.export(model, dummy_input, f\"connect4.onnx\", );\n",
    "\n",
    "    print(f\"{step}: checkpoint saved.\")\n",
    "    if train: model.train();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# TRAINING\n",
    "#\n",
    "log_interval = 10000\n",
    "validation_interval = 50000\n",
    "validation_games = 5000\n",
    "checkpoint_interval = 50000\n",
    "losses = []\n",
    "\n",
    "for _ in range(2000000):\n",
    "    env = Connect4()\n",
    "    games += 1    \n",
    "\n",
    "    episode = connect4.generateEpisode(model)\n",
    "\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for _, _, reward in reversed(episode):\n",
    "        R = reward - gamma * R\n",
    "        returns.insert(0, R)\n",
    "    \n",
    "    states, actions, rewards = zip(*episode)\n",
    "    states_tensor = torch.stack(states)         # [t,126]\n",
    "    actions_tensor = torch.LongTensor(actions)  # [t]\n",
    "    returns_tensor = torch.FloatTensor(returns) # [t]\n",
    "\n",
    "    # normalize returns\n",
    "    returns_tensor = (returns_tensor - returns_tensor.mean()) / returns_tensor.std()\n",
    "\n",
    "    baseline = returns_tensor.mean().detach()\n",
    "\n",
    "    model.train()\n",
    "    action_probs = model(states_tensor) # [t, 7]\n",
    "    chosen_probs = action_probs.gather(1, actions_tensor.unsqueeze(1)) # unsqueeze(1) -> [t,1]\n",
    "    chosen_probs += 1e-8 # for numerical stability\n",
    "    log_probs = torch.log(chosen_probs).squeeze() # [t,1] -> squeeze -> [t]\n",
    "\n",
    "    policy_loss = -(returns_tensor - baseline) * log_probs\n",
    "\n",
    "    entropy = -torch.sum(action_probs * torch.log(action_probs), dim=1)\n",
    "\n",
    "    loss = policy_loss.mean() - entropy_coefficient * entropy.mean()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if games % log_interval == 0:\n",
    "        print(f'{games}: average loss: {sum(losses)/len(losses)}')\n",
    "        losses = []\n",
    "\n",
    "    if games % checkpoint_interval == 0:\n",
    "        checkpoint(games)\n",
    "    if games % validation_interval == 0:\n",
    "        print(f'{games}: validating...')\n",
    "        connect4.validate(model, validation_games)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy gradient training connect 4\n",
    "### one hidden layer with 294 neurons and (norm)tanh\n",
    "\n",
    "| Games     | Mode      | Player    | Wins  | Draws | Losses\n",
    "| :-------: | :-------: | :-------: | :---: | :---: | :-----:\n",
    "| 200,000   | prob      | Red       | -     | -     | -     \n",
    "|           |           | Yellow    | -     | -     | -     \n",
    "|           | determ    | Red       | -     | -     | -     \n",
    "|           |           | Yellow    | -     | -     | -    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
