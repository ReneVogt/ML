{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import importlib\n",
    "import connect4\n",
    "importlib.reload(connect4);\n",
    "from connect4 import Connect4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Hyper parameters\n",
    "# \n",
    "alpha = 0.002\n",
    "gamma = 0.99\n",
    "entropy_coefficient = 2.5\n",
    "stabilizer = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create the model and optimizer\n",
    "# \n",
    "model = nn.Sequential(\n",
    "    nn.Linear(126, 294),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(294, 294),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(294, 7),\n",
    "    nn.Softmax(dim=-1))\n",
    "games = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr = alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load model from checkpoint\n",
    "#\n",
    "games = 6000000\n",
    "cp = torch.load(f'connect4-{games}.nn');\n",
    "model.load_state_dict(cp['model_state_dict']);\n",
    "optimizer.load_state_dict(cp['optimizer_state_dict']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def checkpoint(step):\n",
    "    train = model.training\n",
    "    if train: model.eval();\n",
    "    print(f\"{step}: checkpoint...\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, f'connect4-{step}.nn');\n",
    "\n",
    "    dummy_input = Connect4().state\n",
    "    torch.onnx.export(model, dummy_input, f\"connect4.onnx\", );\n",
    "\n",
    "    print(f\"{step}: checkpoint saved.\")\n",
    "    if train: model.train();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# TRAINING\n",
    "#\n",
    "log_interval = 50000\n",
    "validation_interval = 100000\n",
    "validation_games = 10000\n",
    "checkpoint_interval = 100000\n",
    "target_games = 10000000\n",
    "losses = []\n",
    "\n",
    "for _ in range(target_games-games):\n",
    "    env = Connect4()\n",
    "    games += 1    \n",
    "\n",
    "    episode = connect4.generateEpisode(model)\n",
    "\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for _, _, reward in reversed(episode):\n",
    "        R = reward - gamma * R\n",
    "        returns.insert(0, R)\n",
    "    \n",
    "    states, actions, rewards = zip(*episode)\n",
    "    states_tensor = torch.stack(states)         # [t,126]\n",
    "    actions_tensor = torch.LongTensor(actions)  # [t]\n",
    "    returns_tensor = torch.FloatTensor(returns) # [t]\n",
    "\n",
    "    # normalize returns\n",
    "    returns_tensor = (returns_tensor - returns_tensor.mean()) / (returns_tensor.std() + stabilizer)\n",
    "\n",
    "    baseline = returns_tensor.mean().detach()\n",
    "\n",
    "    model.train()\n",
    "    action_probs = model(states_tensor) # [t, 7]\n",
    "    chosen_probs = action_probs.gather(1, actions_tensor.unsqueeze(1)) # unsqueeze(1) -> [t,1]\n",
    "    chosen_probs += stabilizer\n",
    "    log_probs = torch.log(chosen_probs).squeeze() # [t,1] -> squeeze -> [t]\n",
    "\n",
    "    policy_loss = -(returns_tensor - baseline) * log_probs\n",
    "\n",
    "    entropy = -torch.sum(action_probs * torch.log(action_probs), dim=1)\n",
    "\n",
    "    loss = policy_loss.mean() - entropy_coefficient * entropy.mean()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if games % log_interval == 0:\n",
    "        print(f'{games}: average loss: {sum(losses)/len(losses)}')\n",
    "        losses = []\n",
    "\n",
    "    if games % checkpoint_interval == 0:\n",
    "        checkpoint(games)\n",
    "    if games % validation_interval == 0:\n",
    "        print(f'{games}: validating...')\n",
    "        connect4.validate(model, validation_games)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 126-294-ReLU-294-ReLU-7\n",
    "- alpha: 0.002\n",
    "- gamma: 0.99\n",
    "- entropy_coefficient:\n",
    "    - 1.0\n",
    "    - 2.0 since 4,000,000\n",
    "    - 2.5 since 6,000,000\n",
    "- validating against sampling from the model since 6,000,000\n",
    "\n",
    "#### Results\n",
    "|Games|Loss|Red|Yellow||\n",
    "|--:|:--|:-:|:-:|:-:|\n",
    "| 2,000,000 | -1.9643527344226837 | 99.51 0.00 0.49 | 97.00 0.02 2.98 |\n",
    "| 4,000,000 | -1.9719070252478124 | 99.53 0.01 0.46 | 99.17 0.01 0.82 | entropy coeff **2.0** |\n",
    "| 6,000,000 | -3.9055097838521005 | 99.81 0.00 0.19 | 99.41 0.02 0.57 | entropy coeff **2.5**, sampling opponent |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
