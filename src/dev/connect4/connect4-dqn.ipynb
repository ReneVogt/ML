{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import datetime\n",
    "\n",
    "import torch as T\n",
    "\n",
    "from board import Connect4Board\n",
    "from board2dqn import createStateTensor\n",
    "from agent import Connect4Agent, calculateReward\n",
    "from validation import validate\n",
    "from dqn import exportOnnx\n",
    "\n",
    "def log(message):\n",
    "    print(f\"[{datetime.datetime.now().strftime('%H:%M:%S')}] {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Hyper parameters\n",
    "# \n",
    "lr = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 0.3\n",
    "eps_min = 0.05\n",
    "eps_dec = 1e-7\n",
    "batch_count = 2\n",
    "batch_size = 256\n",
    "memory_size = 128000\n",
    "\n",
    "target_update_interval = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Connect4Agent(\n",
    "    lr = lr, \n",
    "    epsilon = epsilon, \n",
    "    epsilon_end = eps_min, \n",
    "    epsilon_decay = eps_dec,\n",
    "    batch_size = batch_size, \n",
    "    batch_count = batch_count,\n",
    "    memory_size = memory_size,\n",
    "    gamma = gamma,\n",
    "    targetUpdateInterval=target_update_interval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint connect4-30000.\n"
     ]
    }
   ],
   "source": [
    "# load agent from checkpoint\n",
    "agent.loadCheckpoint(f'connect4-30000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:55:36] Starting training for 100000 games.\n",
      "[17:07:51] 5000 games, div: 99.28 / 99.28\n",
      "Average loss (last 9974): 0.009782006812703215, last: 0.006781626492738724, epsilon: 0.29950129999998565\n",
      "[17:20:31] 10000 games, div: 99.10 / 99.11\n",
      "Average loss (last 10000): 0.011139358226303011, last: 0.007518141996115446, epsilon: 0.29900129999997127\n",
      "Checkpoint 'connect4-10000' saved.\n",
      "[17:20:31] Validation:\n",
      "Validation with 500 games per player on 4 processes each, MCTS with 50 games.\n",
      "Player 1: 417 won, 80 lost, 3 draws -> 83.40%, div: 88.20%\n",
      "Player 2: 366 won, 133 lost, 1 draws -> 73.20%, div: 99.20%\n",
      "[17:34:04] 15000 games, div: 99.34 / 99.01\n",
      "Average loss (last 10000): 0.011432169418456033, last: 0.009589195251464844, epsilon: 0.2985012999999569\n",
      "[17:45:36] 20000 games, div: 99.36 / 98.90\n",
      "Average loss (last 10000): 0.010960771030560135, last: 0.01399368979036808, epsilon: 0.2980012999999425\n",
      "Checkpoint 'connect4-20000' saved.\n",
      "[17:45:36] Validation:\n",
      "Validation with 500 games per player on 4 processes each, MCTS with 50 games.\n",
      "Player 1: 369 won, 128 lost, 3 draws -> 73.80%, div: 97.20%\n",
      "Player 2: 354 won, 146 lost, 0 draws -> 70.80%, div: 98.60%\n",
      "[18:01:07] 25000 games, div: 98.46 / 98.68\n",
      "Average loss (last 10000): 0.010472434016503393, last: 0.009123271331191063, epsilon: 0.29750129999992814\n",
      "[18:17:01] 30000 games, div: 99.40 / 98.63\n",
      "Average loss (last 10000): 0.009948336468683556, last: 0.014363685622811317, epsilon: 0.29700129999991376\n",
      "Checkpoint 'connect4-30000' saved.\n",
      "[18:17:01] Validation:\n",
      "Validation with 500 games per player on 4 processes each, MCTS with 50 games.\n",
      "Player 1: 398 won, 101 lost, 1 draws -> 79.60%, div: 99.00%\n",
      "Player 2: 394 won, 104 lost, 2 draws -> 78.80%, div: 98.60%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m env\u001b[38;5;241m.\u001b[39mmove(action)\n\u001b[0;32m     29\u001b[0m next_state \u001b[38;5;241m=\u001b[39m createStateTensor(env)\n\u001b[1;32m---> 30\u001b[0m validMovesMask \u001b[38;5;241m=\u001b[39m \u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m validMovesMask[validMoves] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     32\u001b[0m reward \u001b[38;5;241m=\u001b[39m calculateReward(env)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#\n",
    "# TRAINING\n",
    "#\n",
    "gamesToGo = 100000\n",
    "\n",
    "log_interval = 5000\n",
    "\n",
    "validation_interval = 10000\n",
    "validation_gamesPerPlayer = 1000\n",
    "validation_procsPerPlayer = 8\n",
    "validation_strength = 50\n",
    "\n",
    "lastLoggedGame = 0\n",
    "games = set()\n",
    "allGames = set()\n",
    "\n",
    "log(f\"Starting training for {gamesToGo} games.\")\n",
    "\n",
    "for game in range(1, gamesToGo+1):\n",
    "    env = Connect4Board()\n",
    "        \n",
    "    next_state = createStateTensor(env)\n",
    "    \n",
    "    while not env.Finished:\n",
    "        state = next_state\n",
    "        validMoves = [a for a in range(7) if env.is_valid(a)]\n",
    "        action = agent.getTrainingAction(state, validMoves)\n",
    "        env.move(action)\n",
    "        next_state = createStateTensor(env)\n",
    "        validMovesMask = T.zeros(7, dtype=bool)\n",
    "        validMovesMask[validMoves] = True\n",
    "        reward = calculateReward(env)\n",
    "        agent.store_transition(state, action, next_state, validMovesMask, env.Finished, reward)\n",
    "\n",
    "    games.add(env.gameKey)\n",
    "    allGames.add(env.gameKey)\n",
    "    agent.learn()\n",
    "\n",
    "    if game % log_interval == 0:\n",
    "        log(f'{game} games, div: {100*len(games)/(game+1-lastLoggedGame):.2f} / {100*len(allGames)/(game+1):.2f}')\n",
    "        games.clear()\n",
    "        lastLoggedGame = game\n",
    "        agent.printStats()\n",
    "    if game % validation_interval == 0:\n",
    "        agent.saveCheckpoint(f'connect4-{game}')\n",
    "        log(f'Validation:')\n",
    "        validate(agent.evaluationModel, validation_gamesPerPlayer, validation_procsPerPlayer, validation_strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation with 2000 games per player on 8 processes each, MCTS with 50 games.\n",
      "Player 1: 1572 won, 424 lost, 4 draws -> 78.60%, div: 95.10%\n",
      "Player 2: 1535 won, 459 lost, 6 draws -> 76.75%, div: 97.05%\n"
     ]
    }
   ],
   "source": [
    "validate(agent.evaluationModel, 2000, 8, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportOnnx(agent.evaluationModel, 'connect4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
