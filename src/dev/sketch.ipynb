{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformer\n",
    "import training\n",
    "import importlib\n",
    "importlib.reload(transformer);\n",
    "importlib.reload(training);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Hyper parameter\n",
    "#\n",
    "topic           = 'commits'\n",
    "batch_size      = 64\n",
    "max_iterations  = 2000\n",
    "checkpoint_step = 1000\n",
    "learning_rate   = 1e-4\n",
    "eval_iters      = 200\n",
    "\n",
    "#\n",
    "# Network \n",
    "#\n",
    "transformer.attention_heads_per_block = 8\n",
    "transformer.attention_blocks          = 16\n",
    "transformer.sample_size               = 32     # number of consecutive characters to predict from\n",
    "transformer.embedding_size            = 256    # size of the embedding vectors\n",
    "transformer.dropout                   = 0.2\n",
    "\n",
    "# remember to set t.vocabulary_size when data is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load and tokenize training text.\n",
    "#\n",
    "decoder, tokens = training.initializeTrainingData(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load vocabulary and tokens\n",
    "#\n",
    "decoder, tokens = training.loadTrainingData(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.vocabulary_size = len(decoder)\n",
    "training_data = training.createDataTensors(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Model creation and validation\n",
    "#\n",
    "model = transformer.Transformer()\n",
    "m = model.to(transformer.device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "start_iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load model from checkpoint\n",
    "#\n",
    "start_iteration = 8000\n",
    "checkpoint = torch.load(f'{topic}/{topic}-{start_iteration}.nn');\n",
    "model.load_state_dict(checkpoint['model_state_dict']);\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(step):\n",
    "    train = model.training\n",
    "    if train: model.eval();\n",
    "    print(f\"{step}: checkpoint...\")\n",
    "    losses = training.estimate_loss(model, training_data, eval_iters, transformer.sample_size, batch_size, transformer.device)\n",
    "    print(f\"{step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, f'{topic}/{topic}-{step}.nn');\n",
    "\n",
    "    dummy_input = torch.randint(low=0, high=t.vocabulary_size, size=(1, t.sample_size), dtype=torch.long)\n",
    "    torch.onnx.export(model, dummy_input, f\"{topic}/{topic}-{step}.onnx\");\n",
    "\n",
    "    print(f\"{step}: checkpoint saved.\")\n",
    "    if train: model.train();\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000: training\n",
      "8250: training\n",
      "8500: training\n",
      "8750: training\n",
      "9000: checkpoint...\n",
      "9000: train loss 1.4990, val loss 2.5709\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "9000: checkpoint saved.\n",
      "9000: training\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Training\n",
    "#\n",
    "model.train()\n",
    "for iter in range(max_iterations):\n",
    "    current_iteration = iter + start_iteration\n",
    "    if current_iteration % checkpoint_step == 0 and (start_iteration == 0 or current_iteration > start_iteration):\n",
    "        checkpoint(current_iteration)\n",
    "\n",
    "    if current_iteration % 250 == 0:\n",
    "        print(f\"{current_iteration}: training\")\n",
    "\n",
    "    xb, yb = training.get_batch(training_data['train'], transformer.sample_size, batch_size, transformer.device)\n",
    "    _, loss = model(xb,yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "current_iteration += 1\n",
    "checkpoint(current_iteration)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit message generation training\n",
    "\n",
    "#### Hyper and network parameters\n",
    "| Parameter                 | Value \n",
    "| :--------                 | ----:\n",
    "| tokenizer steps           | 300\n",
    "| sample size               | 32\n",
    "| embedding size            | 256\n",
    "| batch size                | 64\n",
    "| learning rate             | 1e-4\n",
    "| attention heads per block | 8\n",
    "| attention blocks          | 16\n",
    "| dropout ratio             | 0.2\n",
    "\n",
    "\n",
    "#### Training results\n",
    "| Iteration | Loss (training)   | Loss (validation)\n",
    "| :-------: | :-------------:   | :---------------:\n",
    "| 0         | 6.2128            | 6.2222\n",
    "| 1000      | 3.2123            | 3.3551\n",
    "| 2000      | 3.2123            | 3.3551\n",
    "| 3000      | 2.2692            | 2.6352\n",
    "| 4000      | 2.0836            | 2.5631\n",
    "| 5000      | 1.9396            | 2.5280\n",
    "| 6000      | 1.8112            | 2.5055\n",
    "| 7000      | 1.7079            | 2.5406\n",
    "| 8000      | 1.5978            | 2.5683\n",
    "| 9000      | 1.4990            | 2.5709"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Änderung\n",
      "XmlWriter DatabaseInfo und Architektur bei Condition von Auch bereits verarbeitet sollte Matching für IsReleaseWatcher refactored\n",
      "Email von SystemInfos und ArgumentNullValue and implemented code issue\n",
      "* Update radious old config via flow postal\n",
      "- changed version to 1.1.0\n",
      "zip to enjust type to für DeviceTypes (Toolbar wenn es unterstützt Wert\n",
      "Hilfe implementiert  (Fehler und entfernt)\n",
      "ServicesMsi - Checkbox der Mappings erweitert (um result zu esolled)\n",
      "Single Erkennung von UPE-Number wird jetzt aufgetauscht\n",
      "Bug bei Innerhalten von Hinträge es Zeiugriff wieder sich nicht mehr exportiert)\n",
      "setting um ShortNameString() auf UKFD zur added fehlerhaft Fool\n",
      "XOMD.net und FirmwareVersion das Info/AMP Dialog nicht angepasst\n",
      "Reihenfolge: vlt nun die Datenbank zu \n",
      "Schema gefixt\n",
      "Datenbankwänk fertig nun ausgelagert\n",
      "Neue Adapter geprüft und web.config log vorschö\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "decode = lambda l: ''.join([decoder[i] for i in l])\n",
    "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long, device=t.device), max_tokens=500)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
