{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformer\n",
    "import training\n",
    "import importlib\n",
    "importlib.reload(transformer);\n",
    "importlib.reload(training);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Hyper parameter\n",
    "#\n",
    "topic           = 'shakespeare'\n",
    "batch_size      = 192\n",
    "max_iterations  = 1000\n",
    "checkpoint_step = 1000\n",
    "learning_rate   = 1e-4\n",
    "eval_iters      = 200\n",
    "eval_batch_size = 128\n",
    "\n",
    "#\n",
    "# Network \n",
    "#\n",
    "transformer.attention_heads_per_block = 8\n",
    "transformer.attention_blocks          = 16\n",
    "transformer.sample_size               = 128     # number of consecutive characters to predict from\n",
    "transformer.embedding_size            = 384    # size of the embedding vectors\n",
    "transformer.dropout                   = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load vocabulary and tokens\n",
    "#\n",
    "decoder, tokens = training.loadTrainingData(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.vocabulary_size = len(decoder)\n",
    "training_data = training.createDataTensors(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Model creation and validation\n",
    "#\n",
    "model = transformer.Transformer()\n",
    "m = model.to(transformer.device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "start_iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load model from checkpoint\n",
    "#\n",
    "start_iteration = 8000\n",
    "checkpoint = torch.load(f'{topic}/{topic}-{start_iteration}.nn');\n",
    "model.load_state_dict(checkpoint['model_state_dict']);\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(step):\n",
    "    train = model.training\n",
    "    if train: model.eval();\n",
    "    print(f\"{step}: checkpoint...\")\n",
    "    losses = training.estimate_loss(model, training_data, eval_iters, transformer.sample_size, eval_batch_size, transformer.device)\n",
    "    print(f\"{step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, f'{topic}/{topic}-{step}.nn');\n",
    "\n",
    "    dummy_input = torch.randint(low=0, high=transformer.vocabulary_size, size=(1, transformer.sample_size), dtype=torch.long)\n",
    "    torch.onnx.export(model, dummy_input, f\"{topic}/{topic}-{step}.onnx\");\n",
    "\n",
    "    print(f\"{step}: checkpoint saved.\")\n",
    "    if train: model.train();\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_iteration = 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Training\n",
    "#\n",
    "model.train()\n",
    "for iter in range(max_iterations):\n",
    "    current_iteration = iter + start_iteration\n",
    "    if current_iteration % checkpoint_step == 0 and (start_iteration == 0 or current_iteration > start_iteration):\n",
    "        checkpoint(current_iteration)\n",
    "\n",
    "    if current_iteration % 250 == 0:\n",
    "        print(f\"{current_iteration}: training\")\n",
    "\n",
    "    xb, yb = training.get_batch(training_data['train'], transformer.sample_size, batch_size, transformer.device)\n",
    "    _, loss = model(xb,yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "current_iteration += 1\n",
    "checkpoint(current_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000: checkpoint...\n",
      "7000: train loss 0.4510, val loss 3.8113\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "7000: checkpoint saved.\n"
     ]
    }
   ],
   "source": [
    "checkpoint(7000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infinite Shakespeare training\n",
    "\n",
    "#### Hyper and network parameters\n",
    "| Parameter                 | Value \n",
    "| :--------                 | ----:\n",
    "| tokenizer steps           | 2000\n",
    "| sample size               | 128\n",
    "| embedding size            | 384\n",
    "| batch size                | 128\n",
    "| learning rate             | 1e-4\n",
    "| attention heads per block | 8\n",
    "| attention blocks          | 16\n",
    "| dropout ratio             | 0.2\n",
    "\n",
    "\n",
    "#### Training results\n",
    "| Iteration | Loss (training)   | Loss (validation)\n",
    "| :-------: | :-------------:   | :---------------:\n",
    "| 0         | 7.8548            | 7.8398\n",
    "| 1000      | 2.9524            | 3.1708\n",
    "| 2000      | 2.4572            | 2.8868\n",
    "| 3000      | 2.0742            | 2.8427\n",
    "| 4000      | 1.6227            | 2.9373\n",
    "| 5000      | 1.1628            | 3.1853\n",
    "| 6000      | 0.7442            | 3.4916\n",
    "| 7000      | 0.4510            | 3.8113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "decode = lambda l: ''.join([decoder[i] for i in l])\n",
    "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long, device=transformer.device), max_tokens=500)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
